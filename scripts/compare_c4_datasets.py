#!/usr/bin/env python3
import os
import json
import hashlib
from datasets import load_from_disk

import numpy as np
import hashlib

def sha1_list(arr):
    """Hash a list of ints safely (supports int32, >255 values)."""
    a = np.asarray(arr, dtype=np.int32)
    return hashlib.sha1(a.tobytes()).hexdigest()

def compare_metadata(path_old, path_new):
    """Compare dataset_info.json metadata if present."""
    meta_old = os.path.join(path_old, "dataset_info.json")
    meta_new = os.path.join(path_new, "dataset_info.json")

    if not (os.path.exists(meta_old) and os.path.exists(meta_new)):
        print("âš ï¸ One or both metadata files missing â€” skipping metadata comparison.")
        return True

    with open(meta_old) as f:
        old = json.load(f)
    with open(meta_new) as f:
        new = json.load(f)

    print("\nğŸ“Œ METADATA COMPARISON")
    print("- Old:", old)
    print("- New:", new)

    if old == new:
        print("âœ… Metadata matches exactly.")
        return True
    else:
        print("âŒ Metadata differs.")
        return False

def compare_splits(ds_old, ds_new, split):
    """Compare a single split (train or validation)."""
    print(f"\nğŸ” Comparing split: {split}")

    old_len = len(ds_old[split])
    new_len = len(ds_new[split])

    print(f"- Old {split} length: {old_len}")
    print(f"- New {split} length: {new_len}")

    if old_len != new_len:
        print("âŒ Length mismatch!")
        return False

    print("â¡ï¸ Lengths match.")

    # Compare example hashes
    print("â¡ï¸ Computing example hashes...")

    for idx in range(old_len):
        ex_old = ds_old[split][idx]
        ex_new = ds_new[split][idx]

        # Compare input_ids
        hash_old_ids = sha1_list(ex_old["input_ids"])
        hash_new_ids = sha1_list(ex_new["input_ids"])

        if hash_old_ids != hash_new_ids:
            print(f"\nâŒ Mismatch found in {split} at index {idx}")
            print("Old input_ids (first 20):", ex_old["input_ids"][:20])
            print("New input_ids (first 20):", ex_new["input_ids"][:20])
            return False

        # Compare attention mask
        hash_old_mask = sha1_list(ex_old["attention_mask"])
        hash_new_mask = sha1_list(ex_new["attention_mask"])

        if hash_old_mask != hash_new_mask:
            print(f"\nâŒ Attention mask mismatch at index {idx}")
            print("Old mask (first 20):", ex_old["attention_mask"][:20])
            print("New mask (first 20):", ex_new["attention_mask"][:20])
            return False

        # Print progress every 5000 examples
        if idx % 5000 == 0 and idx > 0:
            print(f"  compared {idx}/{old_len} examples...")

    print(f"âœ… Split {split} is IDENTICAL!")
    return True

def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--old_path", type=str, required=True,
                        help="Path to dataset generated by old script")
    parser.add_argument("--new_path", type=str, required=True,
                        help="Path to dataset generated by new script")
    args = parser.parse_args()

    print("\n=== LOADING DATASETS ===")
    ds_old = load_from_disk(args.old_path)
    ds_new = load_from_disk(args.new_path)
    print("Loaded both datasets.")

    # 1. Compare metadata
    compare_metadata(args.old_path, args.new_path)

    # 2. Compare splits
    ok_train = compare_splits(ds_old, ds_new, "train")
    ok_val = compare_splits(ds_old, ds_new, "validation")

    if ok_train and ok_val:
        print("\nğŸ‰ğŸ‰ğŸ‰ FINAL RESULT: DATASETS ARE 100% IDENTICAL ğŸ‰ğŸ‰ğŸ‰")
    else:
        print("\nâŒ FINAL RESULT: DATASETS ARE DIFFERENT âŒ")

if __name__ == "__main__":
    main()
